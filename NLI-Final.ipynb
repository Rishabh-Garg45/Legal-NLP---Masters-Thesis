{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T07:22:35.848877Z",
     "iopub.status.busy": "2026-01-05T07:22:35.848663Z",
     "iopub.status.idle": "2026-01-05T07:25:04.964020Z",
     "shell.execute_reply": "2026-01-05T07:25:04.963338Z",
     "shell.execute_reply.started": "2026-01-05T07:22:35.848860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pip install -U transformers accelerate bitsandbytes torch  # install/upgrade first\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import logging\n",
    "\n",
    "logging.set_verbosity_error()  # quieter\n",
    "\n",
    "HF_TOKEN = \"\"  # prefer env var\n",
    "MODEL_ID = \"khalidrajan/Llama-3.1-8B-Instruct-Legal-NLI\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN, use_fast=True)\n",
    "\n",
    "def load_model():\n",
    "    # Try low-memory safe load first\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            token=HF_TOKEN,\n",
    "            device_map=\"auto\",                # let accelerate dispatch\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            low_cpu_mem_usage=True,           # key flag to avoid partial offload/meta devices\n",
    "            offload_folder=\"/tmp/model_offload\",  # optional: where to offload\n",
    "            offload_state_dict=True,          # helpful for very large models\n",
    "            use_safetensors=True,             # safer & faster if available\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(\"Primary load failed:\", repr(e))\n",
    "\n",
    "    # Fallback: try loading in 4-bit (requires bitsandbytes)\n",
    "    try:\n",
    "        print(\"Attempting 4-bit (bitsandbytes) load as fallback...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            token=HF_TOKEN,\n",
    "            device_map=\"auto\",\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e2:\n",
    "        print(\"4-bit fallback also failed:\", repr(e2))\n",
    "\n",
    "    # Last-resort: load to CPU only (may be slow and may OOM)\n",
    "    try:\n",
    "        print(\"Final fallback: loading onto CPU (may be very slow).\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            token=HF_TOKEN,\n",
    "            torch_dtype=torch.float32,\n",
    "            device_map={\"\": \"cpu\"},\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e3:\n",
    "        print(\"CPU fallback failed:\", repr(e3))\n",
    "        raise RuntimeError(\"All model load attempts failed. See traces above.\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # upgrade accelerate & transformers if you run into weird device_map behaviour:\n",
    "    # pip install -U accelerate transformers\n",
    "    model = load_model()\n",
    "    print(\"Model loaded on devices:\", {k: v.device for k, v in model.named_parameters() if hasattr(v, \"device\")}.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T07:25:13.816612Z",
     "iopub.status.busy": "2026-01-05T07:25:13.816345Z",
     "iopub.status.idle": "2026-01-05T07:25:29.570722Z",
     "shell.execute_reply": "2026-01-05T07:25:29.569996Z",
     "shell.execute_reply.started": "2026-01-05T07:25:13.816591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple\n",
    "\n",
    "# assume `model` and `tokenizer` are already loaded and on the right device\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "LABELS = [\"Entailed\", \"Contradicted\", \"Neutral\"]\n",
    "\n",
    "# Pre-tokenize label strings to token id lists (keeps leading space so tokenization matches generation)\n",
    "label_token_ids = [tokenizer.encode(\" \" + lbl, add_special_tokens=False) for lbl in LABELS]\n",
    "# Show if any label has >1 token (handled below)\n",
    "print(\"Label token lengths:\", {lbl: len(tok) for lbl, tok in zip(LABELS, label_token_ids)})\n",
    "\n",
    "def score_label_sequence(prompt_input_ids: torch.Tensor, label_ids: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Given input_ids for the prompt (1 x L_prompt) and a label token id list,\n",
    "    returns the log-probability of the label sequence being generated next:\n",
    "      log P(label_0, label_1, ... | prompt)\n",
    "    \"\"\"\n",
    "    # concat prompt + label tokens and run model once\n",
    "    concat = torch.cat([prompt_input_ids, torch.tensor(label_ids, device=device).unsqueeze(0)], dim=1)\n",
    "    # get logits for the whole concatenated sequence\n",
    "    with torch.no_grad():\n",
    "        outputs = model(concat)\n",
    "        logits = outputs.logits  # shape (1, seq_len, vocab_size)\n",
    "\n",
    "    # We only need logits at positions corresponding to label tokens:\n",
    "    # those are logits[:, prompt_len-1 : prompt_len+label_len-1] -> but for causal LM, the logit that predicts token t is at index t-1\n",
    "    prompt_len = prompt_input_ids.shape[1]\n",
    "    label_len = len(label_ids)\n",
    "    # logits predicting label token i are at position: prompt_len + i - 1\n",
    "    logprob = 0.0\n",
    "    for i in range(label_len):\n",
    "        logit_pos = prompt_len + i - 1\n",
    "        # if logit_pos < 0 (i==0 and prompt_len==0) handle gracefully (shouldn't happen here)\n",
    "        token_logits = logits[0, logit_pos]  # vocab logits\n",
    "        probs = F.log_softmax(token_logits, dim=-1)\n",
    "        token_id = label_ids[i]\n",
    "        logprob += probs[token_id].item()\n",
    "    return logprob\n",
    "\n",
    "def classify_one(premise: str, hypothesis: str, labels: List[str] = LABELS) -> str:\n",
    "    \"\"\"\n",
    "    Returns the best label from `labels` for a single premise/hypothesis pair.\n",
    "    \"\"\"\n",
    "    system = (\"You are a legal NLI assistant. Given a legal PREMISE and a HYPOTHESIS, \"\n",
    "              \"respond with exactly one word: Entailed, Contradicted, or Neutral.\")\n",
    "    user = f\"PREMISE: {premise}\\nHYPOTHESIS: {hypothesis}\\nLabel:\"\n",
    "    prompt = system + \"\\n\\n\" + user\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    # compute score for each label token sequence\n",
    "    best_label = None\n",
    "    best_score = -1e9\n",
    "    for lbl, lbl_tokens in zip(labels, label_token_ids):\n",
    "        score = score_label_sequence(input_ids, lbl_tokens)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_label = lbl\n",
    "    return best_label\n",
    "\n",
    "# ---- Batched classification (multiple pairs) ----\n",
    "def classify_batch(pairs: List[Tuple[str, str]], labels: List[str] = LABELS, batch_size: int = 4) -> List[str]:\n",
    "    \"\"\"\n",
    "    Classify a list of (premise, hypothesis) pairs.\n",
    "    Returns list of predicted labels in same order.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[i:i+batch_size]\n",
    "        prompts = []\n",
    "        for premise, hypothesis in batch:\n",
    "            system = (\"You are a legal NLI assistant. Given a legal PREMISE and a HYPOTHESIS, \"\n",
    "                      \"respond with exactly one word: Entailed, Contradicted, or Neutral.\")\n",
    "            user = f\"PREMISE: {premise}\\nHYPOTHESIS: {hypothesis}\\nLabel:\"\n",
    "            prompts.append(system + \"\\n\\n\" + user)\n",
    "\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)  # shape (B, L)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device) if \"attention_mask\" in inputs else None\n",
    "\n",
    "        # We'll compute concat model runs for each example separately (since prompt lengths vary)\n",
    "        # Could be optimized by grouping equal-length prompts; this is simpler and safe.\n",
    "        for idx in range(input_ids.shape[0]):\n",
    "            # determine actual prompt length using attention mask or pad token\n",
    "            if attention_mask is not None:\n",
    "                prompt_len = int(attention_mask[idx].sum().item())\n",
    "            else:\n",
    "                # if no mask, find first padding id (heuristic)\n",
    "                seq = input_ids[idx]\n",
    "                prompt_len = seq.ne(seq[0]).sum().item()  # not perfect; better to use mask\n",
    "\n",
    "            single_input_ids = input_ids[idx:idx+1, :prompt_len]  # (1, prompt_len)\n",
    "            best_label = None\n",
    "            best_score = -1e9\n",
    "            for lbl, lbl_tokens in zip(labels, label_token_ids):\n",
    "                score = score_label_sequence(single_input_ids, lbl_tokens)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_label = lbl\n",
    "            results.append(best_label)\n",
    "    return results\n",
    "\n",
    "# ---- tiny demo / eval ----\n",
    "if __name__ == \"__main__\":\n",
    "    demo_pairs = [\n",
    "        (\"The tenant must provide 30 days' written notice before terminating the lease.\",\n",
    "         \"A tenant can end the lease immediately without notifying the landlord.\"),\n",
    "        (\"A party who signs a contract is bound by its terms unless fraud is proven.\",\n",
    "         \"If someone signs a contract, they are never bound by it.\"),\n",
    "        (\"All employees are entitled to one day off weekly according to the policy.\",\n",
    "         \"Employees get at least one day off every week.\"),\n",
    "    ]\n",
    "    # single predictions\n",
    "    for p, h in demo_pairs:\n",
    "        print(\"P:\", p)\n",
    "        print(\"H:\", h)\n",
    "        print(\"->\", classify_one(p, h))\n",
    "        print()\n",
    "\n",
    "    # batched\n",
    "    print(\"Batched results:\", classify_batch(demo_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-05T07:26:33.931430Z",
     "iopub.status.busy": "2026-01-05T07:26:33.930832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# NLI Evaluation on 1000 Random Pairs\n",
    "# ================================\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "DATA_DIR = \"/kaggle/input/lrec-dataset\"\n",
    "POS_FILE = os.path.join(DATA_DIR, \"sentencePair.txt\")\n",
    "NEG_FILE = os.path.join(DATA_DIR, \"sentencePair_neg.txt\")\n",
    "SAMPLE_SIZE = 1000\n",
    "SEED = 42\n",
    "BATCH_SIZE = 2   # safe for 8B model\n",
    "\n",
    "# -------------------------\n",
    "# LABEL MAPPING\n",
    "# -------------------------\n",
    "LABEL_MAP = {\n",
    "    \"SUPPORT\": \"Entailed\",\n",
    "    \"ATTACK\": \"Contradicted\",\n",
    "    \"REFUTE\": \"Contradicted\",\n",
    "    \"NO_REL\": \"Neutral\",\n",
    "    \"NEUTRAL\": \"Neutral\"\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# LOAD + CLEAN DATA\n",
    "# -------------------------\n",
    "def load_sentencepair_file(path):\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        quoting=3,\n",
    "        dtype=str\n",
    "    )\n",
    "    df = df.rename(columns={\n",
    "        3: \"premise\",\n",
    "        6: \"hypothesis\",\n",
    "        8: \"raw_label\"\n",
    "    })\n",
    "    df = df[[\"premise\", \"hypothesis\", \"raw_label\"]]\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "df_pos = load_sentencepair_file(POS_FILE)\n",
    "df_neg = load_sentencepair_file(NEG_FILE)\n",
    "\n",
    "df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "\n",
    "df[\"label\"] = df[\"raw_label\"].map(LABEL_MAP)\n",
    "df = df.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Label distribution (full dataset):\")\n",
    "print(df[\"label\"].value_counts(), \"\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# RANDOM SAMPLE\n",
    "# -------------------------\n",
    "df_sample = df.sample(\n",
    "    n=min(SAMPLE_SIZE, len(df)),\n",
    "    random_state=SEED\n",
    ").reset_index(drop=True)\n",
    "\n",
    "pairs = list(zip(df_sample[\"premise\"], df_sample[\"hypothesis\"]))\n",
    "true_labels = df_sample[\"label\"].tolist()\n",
    "\n",
    "print(f\"Running NLI on {len(pairs)} sentence pairs...\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# NLI INFERENCE\n",
    "# -------------------------\n",
    "pred_labels = classify_batch(\n",
    "    pairs,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# EVALUATION\n",
    "# -------------------------\n",
    "acc = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "print(\"===== Accuracy =====\")\n",
    "print(f\"{acc:.4f}\\n\")\n",
    "\n",
    "print(\"===== Classification Report =====\")\n",
    "print(classification_report(\n",
    "    true_labels,\n",
    "    pred_labels,\n",
    "    labels=[\"Entailed\", \"Contradicted\", \"Neutral\"]\n",
    "))\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    true_labels,\n",
    "    pred_labels,\n",
    "    labels=[\"Entailed\", \"Contradicted\", \"Neutral\"]\n",
    ")\n",
    "\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"True_Entailed\", \"True_Contradicted\", \"True_Neutral\"],\n",
    "    columns=[\"Pred_Entailed\", \"Pred_Contradicted\", \"Pred_Neutral\"]\n",
    ")\n",
    "\n",
    "print(\"===== Confusion Matrix =====\")\n",
    "display(cm_df)\n",
    "\n",
    "# -------------------------\n",
    "# SAVE RESULTS\n",
    "# -------------------------\n",
    "df_sample[\"predicted_label\"] = pred_labels\n",
    "df_sample.to_csv(\"/kaggle/working/nli_eval_1000_pairs.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved predictions to /kaggle/working/nli_eval_1000_pairs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8483803,
     "sourceId": 13372467,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

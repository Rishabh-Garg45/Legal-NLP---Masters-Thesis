{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13372467,"sourceType":"datasetVersion","datasetId":8483803}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# two_model_pipeline_lowmem.py\nimport os\nimport json\nimport time\nimport re\nfrom collections import Counter\nfrom typing import List\nimport gc\n\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm.auto import tqdm\n\n# -------------------------\n# CONFIG\n# -------------------------\nDATA_DIR = \"/kaggle/input/lrec-dataset\"\nFILE_POS = os.path.join(DATA_DIR, \"sentencePair.txt\")\nFILE_NEG = os.path.join(DATA_DIR, \"sentencePair_neg.txt\")\n\nBASE_MODEL_NAME = \"Equall/Saul-7B-Instruct-v1\"\nJUDGE_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\nMAX_SENTENCES = 1000  # or None\nBATCH_SIZE_BASE = 4    # small to avoid OOM\nBATCH_SIZE_JUDGE = 4   # small to avoid OOM\nMAX_NEW_TOKENS_BASE = 64\nMAX_NEW_TOKENS_JUDGE = 64\nMAX_PROMPT_TOKENS = 1024  # truncate long prompts\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nOUTPUT_CSV = \"/kaggle/working/sentence_argument_labels_with_big_judge.csv\"\nCHECKPOINT_INTERVAL = 200\nSEED = 42\n\nCLASS_LABELS = [\"premise\", \"conclusion\", \"non-argumentative\"]\n\ntorch.manual_seed(SEED)\n\nprint(\"Device:\", DEVICE)\nprint(\"Base model:\", BASE_MODEL_NAME)\nprint(\"Judge model:\", JUDGE_MODEL_NAME)\n\n# -------------------------\n# LOAD DATA\n# -------------------------\ncol_names = [\n    \"pair_id\",\n    \"doc1\",\n    \"line1\",\n    \"sent1\",\n    \"doc2\",\n    \"line2\",\n    \"sent2\",\n    \"rel_code\",\n    \"rel_label\",\n]\n\ndf_pos = pd.read_csv(FILE_POS, sep=\"\\t\", header=None, names=col_names, quoting=3, encoding=\"utf-8\")\ndf_neg = pd.read_csv(FILE_NEG, sep=\"\\t\", header=None, names=col_names, quoting=3, encoding=\"utf-8\")\n\ndf_pairs = pd.concat([df_pos, df_neg], ignore_index=True)\nprint(\"Total pairs:\", len(df_pairs))\n\nsent1_df = df_pairs[[\"doc1\", \"line1\", \"sent1\"]].rename(columns={\"doc1\": \"doc\", \"line1\": \"line\", \"sent1\": \"text\"})\nsent2_df = df_pairs[[\"doc2\", \"line2\", \"sent2\"]].rename(columns={\"doc2\": \"doc\", \"line2\": \"line\", \"sent2\": \"text\"})\n\ndf_sentences = (\n    pd.concat([sent1_df, sent2_df], ignore_index=True)\n    .drop_duplicates(subset=[\"doc\", \"line\", \"text\"])\n    .reset_index(drop=True)\n)\ndf_sentences.insert(0, \"sent_id\", range(1, len(df_sentences) + 1))\n\nif MAX_SENTENCES is not None:\n    df_sentences = df_sentences.head(MAX_SENTENCES).reset_index(drop=True)\n\nprint(\"Unique sentences to classify:\", len(df_sentences))\n\n# -------------------------\n# MODEL LOADER\n# -------------------------\ndef load_model_and_tokenizer(name: str):\n    tokenizer = AutoTokenizer.from_pretrained(name, use_fast=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    dtype = torch.float16 if DEVICE == \"cuda\" else torch.float32\n    model = AutoModelForCausalLM.from_pretrained(\n        name,\n        torch_dtype=dtype,\n        device_map=\"auto\" if DEVICE == \"cuda\" else None,\n    )\n    model.eval()\n    return tokenizer, model\n\n# -------------------------\n# PROMPT TEMPLATES (placeholders)\n# -------------------------\nBASE_PROMPT_TEMPLATE = \"\"\"\nYou are an expert in legal argument mining.\n\nTask:\nGiven a SINGLE sentence from a court case, classify it into exactly ONE of these categories:\n\n1. \"premise\"            - a reason, supporting fact, or evidence offered in support of some conclusion.\n2. \"conclusion\"         - a main claim, decision, ruling, or statement that is being supported by reasons.\n3. \"non-argumentative\"  - purely narrative, descriptive, procedural, factual background, citations, headings, etc.\n\nImportant rules:\n- Focus ONLY on the content of the sentence itself.\n- Ignore any dataset labels, case numbers, or line numbers.\n- Output ONLY a single JSON object with this exact schema:\n\n  {\"label\": \"premise\"}            OR\n  {\"label\": \"conclusion\"}         OR\n  {\"label\": \"non-argumentative\"}\n\nDo NOT add explanations, comments, or extra text.\n\nExamples:\n\nSentence:\n\"The defendant was seen leaving the scene of the crime carrying the victim's briefcase.\"\nJSON:\n{\"label\": \"premise\"}\n\nSentence:\n\"Therefore, the defendant is liable to be convicted under Section 420 of the Indian Penal Code.\"\nJSON:\n{\"label\": \"conclusion\"}\n\nSentence:\n\"The present appeal is directed against the judgment dated 12.03.2019 passed by the High Court of Delhi.\"\nJSON:\n{\"label\": \"non-argumentative\"}\n\nNow classify the following sentence.\n\nSentence:\n\\\"\\\"\\\"@@SENT@@\\\"\\\"\\\"\n\nJSON:\n\"\"\".strip()\n\nJUDGE_PROMPT_TEMPLATE = \"\"\"\nYou are a highly capable legal argument mining JUDGE model.\n\nYou are given:\n1. A single sentence from a legal judgment.\n2. A candidate label predicted by a smaller model.\n\nYour job:\n- Analyze the sentence.\n- Decide whether the candidate label is correct.\n- If correct, KEEP it.\n- If wrong, CHANGE it to the best label.\n\nPossible labels (exactly one):\n- \"premise\"\n- \"conclusion\"\n- \"non-argumentative\"\n\nOutput ONLY a single JSON object (no explanation), one of:\n\n  {\"label\": \"premise\"}\n  {\"label\": \"conclusion\"}\n  {\"label\": \"non-argumentative\"}\n\nSentence:\n\\\"\\\"\\\"@@SENT@@\\\"\\\"\\\"\n\nCandidate label:\n\"@@CAND@@\"\n\nJSON:\n\"\"\".strip()\n\n# -------------------------\n# LABEL EXTRACTION\n# -------------------------\n_json_re = re.compile(r\"\\{[^{}]*\\blabel\\b\\s*:\\s*\\\"?([a-zA-Z0-9\\-\\_ ]+)\\\"?[^{}]*\\}\", re.IGNORECASE)\n\ndef normalize_label(raw: str) -> str:\n    if raw is None:\n        return \"non-argumentative\"\n    l = str(raw).strip().lower()\n    if l in {\"no relation\", \"no_relation\", \"no-relation\", \"none\", \"background\", \"\"}:\n        return \"non-argumentative\"\n    if l not in CLASS_LABELS:\n        if \"premise\" in l:\n            return \"premise\"\n        if \"conclusion\" in l:\n            return \"conclusion\"\n        if \"non\" in l or \"none\" in l or \"background\" in l:\n            return \"non-argumentative\"\n        return \"non-argumentative\"\n    return l\n\ndef extract_label_from_text(text: str) -> str:\n    if not isinstance(text, str):\n        return \"non-argumentative\"\n    text = text.strip()\n\n    # 1) regex JSON-like\n    m = _json_re.search(text)\n    if m:\n        try:\n            label = normalize_label(m.group(1))\n            return label\n        except Exception:\n            pass\n\n    # 2) try parse some JSON inside\n    try:\n        start = text.find(\"{\")\n        end = text.rfind(\"}\")\n        if start != -1 and end != -1 and end > start:\n            maybe = text[start:end+1]\n            data = json.loads(maybe.replace(\"'\", '\"'))\n            label = normalize_label(data.get(\"label\", \"\"))\n            return label\n    except Exception:\n        pass\n\n    # 3) heuristics\n    lower = text.lower()\n    if \"premise\" in lower:\n        return \"premise\"\n    if \"conclusion\" in lower:\n        return \"conclusion\"\n    if \"non-argumentative\" in lower or \"no relation\" in lower or \"none\" in lower:\n        return \"non-argumentative\"\n    return \"non-argumentative\"\n\n# -------------------------\n# BATCHED GENERATION\n# -------------------------\ndef generate_batch(\n    prompts: List[str],\n    tokenizer,\n    model,\n    max_new_tokens: int = 64,\n    gen_batch_size: int = 4,\n    max_prompt_tokens: int = 1024,\n):\n    results = []\n    model_device = next(model.parameters()).device\n\n    for i in range(0, len(prompts), gen_batch_size):\n        batch_prompts = prompts[i : i + gen_batch_size]\n        enc = tokenizer(\n            batch_prompts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_prompt_tokens,\n        ).to(model_device)\n\n        with torch.inference_mode():\n            gen_ids = model.generate(\n                **enc,\n                max_new_tokens=max_new_tokens,\n                do_sample=False,       # greedy\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n            )\n\n        decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n        for orig, full in zip(batch_prompts, decoded):\n            if full.startswith(orig):\n                comp = full[len(orig):].strip()\n            else:\n                comp = full.strip()\n            results.append(comp)\n\n    return results\n\n# -------------------------\n# PIPELINE\n# -------------------------\nsentences = df_sentences[\"text\"].astype(str).tolist()\nn = len(sentences)\n\nbase_labels = [None] * n\nfinal_labels = [None] * n\n\n# -------------------------\n# STEP 1: BASE MODEL (only base in GPU)\n# -------------------------\nprint(\"Loading base model...\")\ntokenizer_base, model_base = load_model_and_tokenizer(BASE_MODEL_NAME)\n\nbase_prompts = [BASE_PROMPT_TEMPLATE.replace(\"@@SENT@@\", s) for s in sentences]\n\nprint(\"Running base model in batches...\")\nfor start in tqdm(range(0, n, BATCH_SIZE_BASE)):\n    end = min(n, start + BATCH_SIZE_BASE)\n    batch_prompts = base_prompts[start:end]\n\n    outputs = generate_batch(\n        batch_prompts,\n        tokenizer_base,\n        model_base,\n        max_new_tokens=MAX_NEW_TOKENS_BASE,\n        gen_batch_size=BATCH_SIZE_BASE,\n        max_prompt_tokens=MAX_PROMPT_TOKENS,\n    )\n\n    for idx_in_batch, out in enumerate(outputs):\n        idx = start + idx_in_batch\n        lbl = extract_label_from_text(out)\n        base_labels[idx] = lbl\n\nprint(\"Base pass done. Distribution:\", Counter(base_labels))\n\n# Free base model from GPU\ndel model_base\ndel tokenizer_base\ngc.collect()\nif DEVICE == \"cuda\":\n    torch.cuda.empty_cache()\nprint(\"Freed base model from GPU.\")\n\n# -------------------------\n# STEP 2: JUDGE MODEL (load only now)\n# -------------------------\nprint(\"Loading judge model...\")\ntokenizer_judge, model_judge = load_model_and_tokenizer(JUDGE_MODEL_NAME)\n\njudge_prompts = [\n    JUDGE_PROMPT_TEMPLATE.replace(\"@@SENT@@\", s).replace(\"@@CAND@@\", base_labels[i])\n    for i, s in enumerate(sentences)\n]\n\nprint(\"Running judge model in batches...\")\nfor start in tqdm(range(0, n, BATCH_SIZE_JUDGE)):\n    end = min(n, start + BATCH_SIZE_JUDGE)\n    batch_prompts = judge_prompts[start:end]\n\n    outputs = generate_batch(\n        batch_prompts,\n        tokenizer_judge,\n        model_judge,\n        max_new_tokens=MAX_NEW_TOKENS_JUDGE,\n        gen_batch_size=BATCH_SIZE_JUDGE,\n        max_prompt_tokens=MAX_PROMPT_TOKENS,\n    )\n\n    for idx_in_batch, out in enumerate(outputs):\n        idx = start + idx_in_batch\n        judged = extract_label_from_text(out)\n        final_labels[idx] = judged if judged in CLASS_LABELS else base_labels[idx]\n\n    # periodic checkpoint\n    if (start // BATCH_SIZE_JUDGE) % max(1, (CHECKPOINT_INTERVAL // BATCH_SIZE_JUDGE)) == 0:\n        tmp = df_sentences.copy()\n        tmp[\"base_llm_label\"] = base_labels\n        tmp[\"final_llm_label\"] = final_labels\n        tmp.to_csv(OUTPUT_CSV + \".partial\", index=False, encoding=\"utf-8\")\n        print(f\"Intermediate saved up to {end}/{n}\")\n\nprint(\"Judge pass done.\")\n\n# Optional: free judge model\ndel model_judge\ndel tokenizer_judge\ngc.collect()\nif DEVICE == \"cuda\":\n    torch.cuda.empty_cache()\nprint(\"Freed judge model from GPU.\")\n\n# -------------------------\n# SAVE RESULTS\n# -------------------------\ndf_sentences[\"base_llm_label\"] = base_labels\ndf_sentences[\"final_llm_label\"] = final_labels\n\nprint(\"Final distributions (base):\", Counter(base_labels))\nprint(\"Final distributions (final):\", Counter(final_labels))\n\ndf_sentences.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\nprint(\"Saved:\", OUTPUT_CSV)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:14:46.232670Z","iopub.execute_input":"2025-12-09T18:14:46.232963Z","iopub.status.idle":"2025-12-09T18:47:13.563673Z","shell.execute_reply.started":"2025-12-09T18:14:46.232910Z","shell.execute_reply":"2025-12-09T18:47:13.562814Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nBase model: Equall/Saul-7B-Instruct-v1\nJudge model: mistralai/Mistral-7B-Instruct-v0.2\nTotal pairs: 40506\nUnique sentences to classify: 1000\nLoading base model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e27acc1ca114a479a595608e83b5653"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7832b6bb5c5c4c7eb0407ea8beb894a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e162ade1d4e145b9950a715c5098a86d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c596ed1cbf194bd09386c30d104b5615"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"908cf351637c49baac76124215d5902d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-12-09 18:15:09.629100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765304109.820324      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765304109.870797      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1fa973b25147c2887521551a749c19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb5c32682cd6414b926def519a93946f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00006.safetensors:   0%|          | 0.00/4.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e0af9ec5da45b991ef7f6fe674d07c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00006.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86454d170a964ad3ab849886321b9694"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00006.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc332f36c99545acafe6e075ba802ecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00006.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"218a7488665b4a759154264715efaf43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00006.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6183662e7fd04cfd83b48b69d90b3d6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00006.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"422bee544ada408c8b1a884e553c385b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"929859bbaa4c4972aef700a3f694cbfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9158945fbab64859a5d8f342633f155e"}},"metadata":{}},{"name":"stdout","text":"Running base model in batches...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b38877a41dd47f5b26eabae4c620e3c"}},"metadata":{}},{"name":"stdout","text":"Base pass done. Distribution: Counter({'non-argumentative': 585, 'premise': 301, 'conclusion': 114})\nFreed base model from GPU.\nLoading judge model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e5d8d5770a64a609499040a426eeb61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efa4a610748c4ffeb3c658cce20bfa0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4065cdb0f8d24742a6aabd48091616f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaf03f50b58d4d82b0884550658840e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a551c1f066c1404eb4ed4ea19bac8e9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c98df699b74543679e20f76c2ca4d701"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a8024592f604bca9dfcf713719fd5b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caf1e1c297c14fedbf3aceac9da7908e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1662834948c44973a2f5a0bb7da91a86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e638cd354a74550880039e0032a6eee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b5d73d485264c60a5c53ed4a3de0c94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca5f68686d9a4400a8b925a7a8691535"}},"metadata":{}},{"name":"stdout","text":"Running judge model in batches...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fba3421743e437fa55060463dcfa8e3"}},"metadata":{}},{"name":"stdout","text":"Intermediate saved up to 4/1000\nIntermediate saved up to 204/1000\nIntermediate saved up to 404/1000\nIntermediate saved up to 604/1000\nIntermediate saved up to 804/1000\nJudge pass done.\nFreed judge model from GPU.\nFinal distributions (base): Counter({'non-argumentative': 585, 'premise': 301, 'conclusion': 114})\nFinal distributions (final): Counter({'premise': 714, 'conclusion': 189, 'non-argumentative': 97})\nSaved: /kaggle/working/sentence_argument_labels_with_big_judge.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
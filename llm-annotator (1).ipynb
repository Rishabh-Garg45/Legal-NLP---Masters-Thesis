{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13372467,"sourceType":"datasetVersion","datasetId":8483803},{"sourceId":13772751,"sourceType":"datasetVersion","datasetId":8765733}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T04:31:56.117237Z","iopub.execute_input":"2025-11-18T04:31:56.117531Z","iopub.status.idle":"2025-11-18T04:31:56.124778Z","shell.execute_reply.started":"2025-11-18T04:31:56.117505Z","shell.execute_reply":"2025-11-18T04:31:56.123884Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/lrec-dataset/sentencePair.txt\n/kaggle/input/lrec-dataset/sentencePair_neg.txt\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# pip install -q openai pandas tenacity\n\nimport os, json, math\nfrom typing import List, Dict, Any\nimport pandas as pd\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom openai import OpenAI\n\nMODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\nTEMPERATURE = float(os.getenv(\"TEMPERATURE\", \"0.0\"))\nMAX_RETRIES = int(os.getenv(\"MAX_RETRIES\", \"5\"))\n\nSYSTEM_PROMPT = \"\"\"You are an expert annotator for argument mining in legal and formal texts.\nClassify EACH input sentence as exactly ONE of:\n- Premise\n- Conclusion\n- Non-argumentative\nReturn JSON:\n{\"items\":[{\"text\":\"...\",\"label\":\"Premise|Conclusion|Non-argumentative\",\"confidence\":0-1}, ...]}\n(One label per sentence; see earlier guidelines.)\"\"\"\n\ndef _load_sentences_from_pairs(paths: List[str]) -> List[str]:\n    sentences = []\n    for p in paths:\n        df = pd.read_csv(p, sep=\"\\t\", header=None, dtype=str, on_bad_lines=\"skip\", quoting=3, engine=\"python\")\n        # Try common columns (3 and 6), fall back to all columns as text\n        cols = []\n        if df.shape[1] >= 4: cols.append(3)\n        if df.shape[1] >= 7: cols.append(6)\n        if not cols: cols = list(range(df.shape[1]))\n        for c in cols:\n            sentences.extend(df.iloc[:, c].dropna().astype(str).tolist())\n    # unique & stripped\n    seen, uniq = set(), []\n    for s in sentences:\n        s = s.strip()\n        if s and s not in seen:\n            seen.add(s); uniq.append(s)\n    return uniq\n\nclass JsonShapeError(Exception): pass\n\n@retry(reraise=True, stop=stop_after_attempt(MAX_RETRIES),\n       wait=wait_exponential(min=2, max=30),\n       retry=retry_if_exception_type((JsonShapeError, Exception)))\ndef _classify_batch(client: OpenAI, batch: List[str]) -> List[Dict[str, Any]]:\n    resp = client.chat.completions.create(\n        model=MODEL, temperature=TEMPERATURE,\n        response_format={\"type\": \"json_object\"},\n        messages=[\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": json.dumps({\"sentences\": batch}, ensure_ascii=False)}\n        ],\n    )\n    data = json.loads(resp.choices[0].message.content)\n    if \"items\" not in data or not isinstance(data[\"items\"], list):\n        raise JsonShapeError(\"Missing 'items' list in response\")\n    norm = {\"premise\":\"Premise\",\"conclusion\":\"Conclusion\",\"non-argumentative\":\"Non-argumentative\"}\n    out = []\n    for it in data[\"items\"]:\n        text = str(it.get(\"text\",\"\")).strip()\n        label = str(it.get(\"label\",\"\")).strip()\n        label = norm.get(label.lower(), label if label in norm.values() else \"Non-argumentative\")\n        try:\n            conf = float(it.get(\"confidence\", None))\n            conf = max(0.0, min(1.0, conf))\n        except Exception:\n            conf = None\n        if text:\n            out.append({\"text\": text, \"label\": label, \"confidence\": conf})\n    if not out: raise JsonShapeError(\"Empty items\")\n    return out\n\ndef classify_sentences(\n    files: List[str],\n    out_csv: str = \"sentence_classifications.csv\",\n    batch_size: int = 25,\n):\n    \"\"\"\n    Notebook-friendly driver. Example:\n      classify_sentences(\n          files=[\"/kaggle/input/your-dataset/sentencePair.txt\",\n                 \"/kaggle/input/your-dataset/sentencePair_neg.txt\"],\n          out_csv=\"/kaggle/working/sentence_classifications.csv\",\n          batch_size=25\n      )\n    \"\"\"\n    client = OpenAI()  # requires OPENAI_API_KEY in env\n    sentences = _load_sentences_from_pairs(files)\n    print(f\"Loaded {len(sentences)} unique sentences from {len(files)} file(s).\")\n\n    results = []\n    # Simple batching (no cache for brevity here; add if you like)\n    for i in range(0, len(sentences), batch_size):\n        batch = sentences[i:i+batch_size]\n        print(f\"Batch {i//batch_size + 1} / {math.ceil(len(sentences)/batch_size)} ({len(batch)} sentences)\")\n        items = _classify_batch(client, batch)\n        # map by text in case of reordering\n        mapped = {it[\"text\"]: it for it in items}\n        for s in batch:\n            results.append(mapped.get(s, {\"text\": s, \"label\": \"Non-argumentative\", \"confidence\": None}))\n\n    df = pd.DataFrame(results, columns=[\"text\",\"label\",\"confidence\"])\n    df.to_csv(out_csv, index=False)\n    print(f\"Saved {len(df)} rows to {out_csv}\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T04:41:22.482278Z","iopub.execute_input":"2025-11-18T04:41:22.482569Z","iopub.status.idle":"2025-11-18T04:41:22.501082Z","shell.execute_reply.started":"2025-11-18T04:41:22.482548Z","shell.execute_reply":"2025-11-18T04:41:22.500277Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"df = classify_sentences(\n    files=[\n        \"//kaggle/input/new-data/sanity_50_sentences.csv\",\n    ],\n    out_csv=\"/kaggle/working/sentence_classifications.csv\",\n    batch_size=51\n)\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T04:41:26.595065Z","iopub.execute_input":"2025-11-18T04:41:26.595369Z","execution_failed":"2025-11-18T06:48:43.240Z"}},"outputs":[{"name":"stdout","text":"Loaded 51 unique sentences from 1 file(s).\nBatch 1 / 1 (51 sentences)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}